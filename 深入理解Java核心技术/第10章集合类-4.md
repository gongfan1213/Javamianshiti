
也就是说，如果我们设置的默认值是7，经过JDK处理之后，HashMap的容量会被设置成8，但是，这个HashMap在元素个数达到8×0.75 = 6时就会进行一次扩容，这明显是我们不希望见到的。

那么，到底设置成什么值比较合理呢？

这里我们可以参考JDK8中putAll方法的实现，这个实现在Guava（21.0版本）中也被采用。

这个值的计算方法如下：
```java
return (int) ((float) expectedSize / 0.75F + 1.0F);
```

比如我们计划向HashMap中放入7个元素，通过expectedSize/0.75F + 1.0F计算，7/0.75 + 1 = 10，10经过JDK处理之后，会被设置成16，这就大大减少了扩容的概率。

当HashMap内部维护的Hash表的容量达到75%时（默认情况下），会触发rehash，而rehash的过程是比较耗费时间的。所以初始化容量要设置成expectedSize/0.75 + 1，既可以有效地减少冲突，也可以减小误差。

所以当我们明确知道HashMap中元素的个数时，把默认容量设置成expectedSize/0.75F + 1.0F是一个在性能上相对好的选择，但同时会 “牺牲” 一些内存。

这个算法在Guava中也有实现，可以直接通过Maps类创建一个HashMap：
```java
Map<String, String> map = Maps.newHashMapWithExpectedSize(7);
```
其代码实现如下：
```java
public static <K, V> HashMap<K, V> newHashMapWithExpectedSize(int expectedSize) {
    return new HashMap(capacity(expectedSize));
}
static int capacity(int expectedSize) {
    if (expectedSize < 3) {
        CollectPreconditions.checkNonnegative(expectedSize, "expectedSize");
        return expectedSize + 1;
    } else {
        return expectedSize < 1073741824 ? (int) ((float) expectedSize / 0.75F + 1.0F) : 2147483647;
    }
}
```

但是，以上操作是一种用内存换性能的做法，真正使用的时候，还要考虑内存的影响。

### 小结

当我们想要在代码中创建一个HashMap时，如果已知这个Map中即将存放的元素个数，给HashMap设置初始容量可以在一定程度上提升效率。

但是，JDK并不会直接以用户传进来的数字作为默认容量，而是会进行一番运算，最终得到一个2的幂。得到这个数字的算法其实是使用了无符号右移和按位或运算来提升效率。

为了最大限度地避免扩容带来的性能消耗，建议把默认容量的数字设置成expectedSize/0.75F + 1.0F。在日常开发中，可以使用Map<String, String> map = Maps.newHashMapWithExpectedSize(10)来创建一个HashMap，Guava会帮助我们完成计算的过程。

### 10.16 HashMap的hash()方法

在同一个版本的JDK中，HashMap、HashTable和ConcurrentHashMap中的hash()方法的实现是不同的。在不同的版本的JDK（Java7和Java8）中也是有区别的。

hash()方法的输入应该是一个Object类型的Key，输出应该是一个int类型的数组下标。如果让你设计这个hash()方法，你会怎么做呢？

我们调用Object对象的hashCode()方法，该方法会返回一个整数，然后使用这个数对HashMap或者HashTable的容量进行取模。在具体实现上，由两个方法int hash(Object k)和int indexFor(int h, int length)来实现。考虑到效率等问题，HashMap的实现会稍微复杂一点。
- hash()方法：该方法主要是将Object转换成一个整型值。
- indexFor()方法：该方法主要是将hash()方法生成的整型值转换成链表数组中的下标。
#### HashMap In Java 7
下面看一下Java 7中hash()方法的实现：
```java
final int hash(Object k) {
    int h = hashSeed;
    if (0 != h && k instanceof String) {
        return sun.misc.Hashing.stringHash32((String) k);
    }
    h ^= k.hashCode();
    h ^= (h >>> 20) ^ (h >>> 12);
    return h ^ (h >>> 7) ^ (h >>> 4);
}
static int indexFor(int h, int length) {
    return h & (length - 1);
}
```
前面说过，indexFor方法主要是将hash()方法生成的整型值转换成链表数组中的下标。那么return h & (length - 1)是什么意思呢？其实，这个操作就是取模。之所以使用位运算（&）来代替取模运算（%），最主要的考虑就是效率。位运算（&）的效率要比取模运算（%）高得多，主要原因是位运算直接对内存数据进行操作，不需要转成十进制数据，因此处理速度非常快。
为什么可以使用位运算（&）来实现取模运算（%）呢？其实现的原理如下：
X % 2^n = X & (2^n - 1)
2^n表示2的n次方，也就是说，一个数对2^n取模 = 一个数和（2^n - 1）做按位与运算。
假设n为3，则2^3 = 8，表示为二进制值就是1000。2^3 - 1 = 7，即0111。
此时X & (2^3 - 1)就相当于取X的二进制值的最后三位数。
从二进制角度来看，X/8相当于X >> 3，即把X右移3位，此时得到了X/8的商，而被移掉的部分（后三位）则是X % 8，也就是余数。
下面通过具体的示例来详细解释，如图10 - 13所示。
```
6 % 8 = 6，6 & 7 = 6
10 & 8 = 2，10 & 7 = 2
```
![image](https://github.com/user-attachments/assets/6792a9c7-ae70-4278-91f7-220c6dacee27)



基于return h & (length - 1)，只要保证length的长度是2^n，就可以实现取模运算。而前面介绍过，HashMap中的length也确实是2的倍数，初始值是16，之后每次扩充为原来的2倍。

HashMap的数据是存储在链表数组中的。在对HashMap进行插入/删除等操作时，都需要根据键值对的键值定位到其应该保存在数组的哪个下标中。而这个通过键值求取下标的操作就叫作Hash。HashMap的数组是有长度的，Java中规定这个长度只能是2的倍数，初始值为16。简单的做法是先求取出键值的hashCode，然后将hashCode得到的int值对数组长度进行取模。为了考虑性能，Java采用按位与操作实现取模操作。

无论使用取模运算还是位运算，都无法直接解决冲突较大的问题。比如CA110000和00010000，在与00001111进行按位与运算后的值是相等的。

![image](https://github.com/user-attachments/assets/d22035fa-005a-4c9c-99d1-5333a91f106e)


两个不同的键值在对数组长度进行按位与运算后得到的结果相同，这不就发生了冲突吗？如何解决这种冲突呢？Java的解决方案的主要代码如下：
```java
h ^= k.hashCode();
h ^= (h >>> 20) ^ (h >>> 12);
return h ^ (h >>> 7) ^ (h >>> 4);
```

这段代码是为了对Key的hashCode进行扰动计算，防止不同hashCode的高位不同但低位相同导致的Hash冲突。简单地说，就是为了把高位的特征和低位的特征组合起来，降低Hash冲突的概率，尽量做到任何一位的变化都能对最终得到的结果产生影响。

举个例子，我们现在向一个HashMap中 “put” 一个K - V对，Key的值为hollischuang ，简单地获取hashCode后，得到的值为 “101100011010111001111101001011” ，如果当前HashTable的大小为16，即在不进行扰动计算的情况下，其最终得到的index结果值为11。由于15的二进制值扩展到32位为 “00000000000000000000000000001111” ，所以，一个数字在和它进行按位与运算时，前28位无论是什么，计算结果都一样（因为0和任何数做与运算的结果都为0）。

![image](https://github.com/user-attachments/assets/93e96bcb-0c48-44e3-8c25-8be19defbef2)


可以看到，后面的两个hashCode经过位运算之后得到的值也是11，图10 - 15中的后两个hashCode是笔者自编的，虽然我们不知道哪个Key的hashCode是这两个值，但肯定存在这样的Key，这时候就产生了冲突。

接下来分析经过扰动计算后最终的计算结果是什么。

从图10 - 16中可以看到，之前会产生冲突的两个hashCode经过扰动计算后，最终得到的index的值不一样了，这样就很好地避免了冲突。

![image](https://github.com/user-attachments/assets/c49976e5-1eb9-453d-8a11-c9846609238c)



其实，使用位运算代替取模运算，除了提升了性能，还有一个好处就是可以很好地解决负数的问题。

hashCode的结果是int类型，而int的取值范围是-2^31 ~ 2^31 - 1，即[-2147483648,2147483647]；其中是包含负数的，对一个负数取模还是有些麻烦的。如果使用二进制的位运算，则可以很好地避免这个问题。

首先，不管hashCode的值是正数还是负数。length - 1这个值一定是一个正数。它的二进制值的第一位一定是0（有符号数用最高位作为符号位，“0” 代表 “+” ，“1” 代表 “-” ），这样两个数做按位与运算之后，第一位一定是0，也就是说，得到的结果一定是个正数。

#### HashTable In Java 7

上面是Java 7中HashMap的hash()方法及indexOf方法的实现，接下来我们分析线程安全的HashTable是如何实现的，和HashMap有何不同，并分析具体的原因。以下是Java 7中HashTable的hash()方法的实现：
```java
private int hash(Object k) {
    // hashSeed will be zero if alternative hashing is disabled.
    return hashSeed ^ k.hashCode();
}
```

以上代码相当于只是对k做了一个简单的Hash运算，取了一下其hashCode。而HashTable中也没有indexOf方法，取而代之的是这段代码：int index = (hash & 0x7FFFFFFF) % tab.length。也就是说，HashMap和HashTable采用了两种方法来计算数组下标。HashMap采用的是位运算，而HashTable采用的是直接取模。

为什么要把Hash值和0x7FFFFFFF做一次按位与操作呢？主要是为了保证得到的index的第一位为0，也就是为了得到一个正数。因为有符号数的第一位0代表正数，1代表负数。

前面说过，HashMap之所以不用取模的原因是为了提高效率。有人认为，因为HashTable是一个线程安全的类，本来就慢，所以Java并没有考虑效率问题，直接使用取模算法。

其实，HashTable采用简单的取模算法是出于一定原因考虑的。这就要涉及HashTable的构造函数和扩容函数了。由于篇幅有限，这里就不贴代码了，直接给出结论：

HashTable默认的初始大小为11，之后每次扩充为原来的2n + 1倍。

也就是说，HashTable的链表数组的默认大小是一个素数、奇数，之后的每次扩充结果也都是奇数。

由于HashTable会尽量使用素数、奇数作为容量的大小，所以当Hash表的大小为素数时，简单的取模算法的结果会更加均匀。

至此，我们分析了Java 7中HashMap和HashTable对于hash()方法的实现，下面做一个简单的总结。
- HashMap默认的初始化大小为16，之后每次扩充为原来的2倍。
- HashTable默认的初始大小为11，之后每次扩充为原来的2n + 1倍。
- 当Hash表的大小为素数时，简单的取模Hash的结果会更加均匀，所以单从这一点上看，HashTable的Hash表的容量大小的选择上似乎更高明一些。因为Hash结果越分散效果越好。
- 在取模计算时，如果模数是2的幂，那么可以直接使用位运算来得到结果，效率要大大高于做除法。所以从Hash计算的效率上看，又是HashMap更胜一筹。
- HashMap为了提高效率使用位运算代替Hash运算，这又引入了Hash分布不均匀的问题。HashMap为了解决这个问题，又对Hash算法做了一些改进，增加了扰动计算。
#### ConcurrentHashMap In Java 7
下面这段关于ConcurrentHashMap的Hash实现其实和HashMap如出一辙，都是先通过位运算代替取模，再对hashCode进行扰动计算。区别在于，ConcurrentHashMap使用了一种变种的Wang/Jenkins Hash算法，主要是为了把高位和低位组合在一起，避免发生冲突。
```java
private int hash(Object k) {
    int h = hashSeed;
    if ((0 != h) && (k instanceof String)) {
        return sun.misc.Hashing.stringHash32((String) k);
    }
    h ^= k.hashCode();
    // Spread bits to regularize both segment and index locations,
    // using variant of single-word Wang/Jenkins Hash.
    h += (h << 15) ^ 0xffffcd7d;
    h ^= (h >>> 10);
    h += (h << 3);
    h ^= (h >>> 6);
    h += (h << 2) + (h << 14);
    return h ^ (h >>> 16);
}
int j = (hash >>> segmentShift) & segmentMask;
```


### HashMap In Java 8

在Java 8之前，HashMap和其他基于Map的类都是通过链地址法解决冲突的，它们使用单向链表来存储相同索引值的元素。在最坏的情况下，这种方式会将HashMap的get方法的性能从O(1)降低到O(n)。为了解决在频繁冲突时HashMap性能降低的问题，Java 8使用平衡树替代链表来存储冲突的元素。这意味着我们可以将最坏情况下的性能从O(n)提高到O(logn)。

如果恶意程序知道我们使用的是Hash算法，那么在纯链表情况下，它能够发送大量请求导致Hash碰撞，然后不停访问这些Key导致HashMap忙于进行线性查找，最终陷入瘫痪，即形成拒绝服务攻击（DoS）。

Java 8中的Hash函数的实现原理和Java 7中的基本类似。Java 8中做了优化，只做一次16位右位移和异或混合操作，而不是四次，但原理是不变的，代码如下：

```java
static final int hash(Object key) {
    int h;
    return (key == null)? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```
在JDK1.8的实现中，优化了高位运算的算法，即通过hashCode()的高16位和低16位做 “异或” 操作：(h = k.hashCode()) ^ (h >>> 16)。之所以这么做，主要是从速度、功效、质量等方面综合考虑的。以上方法得到int的Hash值，再通过h & (table.length - 1)得到该对象在数据中保存的位置。
### HashTable In Java 8
在Java 8的HashTable中，已经不再有hash()方法了。但Hash的操作还是存在的，比如在put方法中就有如下实现：
```java
int hash = key.hashCode();
int index = (hash & 0x7FFFFFFF) % tab.length;
```
### ConcurrentHashMap In Java 8
Java 8中的求Hash值的方法从hash()改为了spread()。实现方式如下：
```java
static final int spread(int h) {
    return (h ^ (h >>> 16)) & HASH_BITS;
}
```

Java 8的ConcurrentHashMap同样是通过Key的Hash值与数组长度取模确定该Key在数组中的索引的。为了避免不太好的Key的hashCode设计，它通过前述方法得到Key的最终Hash值。不同的是，Java 8的ConcurrentHashMap的作者认为引入红黑树后，即使Hash冲突比较严重，寻址效率也足够高，所以作者并未在Hash值的计算上做过多设计，只是将Key的hashCode值与其高16位做异或计算并保证最高位为0（从而保证最终结果为正整数）。

### 小结

至此，我们已经分析了HashMap、HashTable和ConcurrentHashMap分别在JDK 1.7和JDK1.8中的实现。可以发现，为了保证Hash的结果可以分散，为了提高Hash的效率，JDK在一个小小的hash()方法上就做了很多事情。当然，我们不仅要深入了解背后的原理，还要学习这种对代码精益求精的态度。

### 10.17 为什么HashMap的默认容量设置成16

在介绍HashMap的基础概念时，还有两个HashMap中的常量没有介绍，即DEFAULT_INITIAL_CAPACITY和DEFAULT_LOAD_FACTOR，分别表示默认容量和默认负载因子。接下来介绍这两个概念。

通过查看源码，可以知道HashMap的默认容量为16：

```java
/**
 * The default initial capacity - MUST be a power of two.
 */
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16
```

我们在介绍HashMap的hash()方法的时候，曾经提到过：

因为位运算是直接对内存数据进行操作，不需要转成十进制，所以位运算要比取模运算的效率更高，HashMap在计算元素要存放在数组中的index时，使用位运算代替了取模运算。之所以可以做等价代替，前提要求HashMap的容量一定是2的n次方。

既然是2的n次方，为什么一定是16呢？为什么不能是4、8或者32呢？

关于这个默认容量的选择，JDK并没有给出官方解释。

根据笔者的推断，这个值应该是个经验值（Experience Value），既然一定要设置一个默认的2的n次方作为初始值，那么就需要在效率和内存使用上做一个权衡。这个值既不能太小，也不能太大。太小了就有可能频繁发生扩容，影响效率。太大了又浪费空间，不划算。所以，16就作为一个经验值被采用了。

在JDK 8中，默认容量的定义为1<<4，其故意把16写成1<<4，就是提醒开发者，这个地方是2的n次方。
HashMap在初始化的时候，把默认值设置成16，这就保证了在用户没有指定初始化容量时，容量会被设置成16，这就满足了容量是2的幂次这一要求。

如果用户指定了一个初始容量，比如指定初始容量为7，会发生什么呢？

HashMap在两个可能改变其容量的地方都做了兼容处理，分别是指定容量初始化时及扩容时。

在初始化容量时，如果用户指定了容量，那么HashMap会采用第一个大于这个数的2的幂作为初始容量。

在扩充容量时，HashMap会把容量扩充到当前容量的2倍。2的幂的2倍，还是2的幂。

通过保证初始化容量均为2的幂，并且扩容时也是扩容到之前容量的2倍，保证了HashMap的容量永远都是2的幂。

### 10.18 为什么HashMap的默认负载因子设置成0.75

本节介绍HashMap中的另一个常量——DEFAULT_LOAD_FACTOR，也就是默认负载因子。

通过查看源码，可以知道HashMap的默认负载因子为0.75：

```java

/**
 * The load factor used when none specified in constructor.
 */
static final float DEFAULT_LOAD_FACTOR = 0.75f;
```

为什么要设置成这个值呢？我们可以随便修改它吗？

前面我们介绍过负载因子（loadFactory），这里再简单回顾一下：

我们知道，第一次创建HashMap时，就会指定其容量，随着我们不断地向HashMap中 “put” 元素，就有可能超过其容量，这时就需要有一个扩容机制。

从代码中我们可以看到，在向HashMap添加元素的过程中，如果元素个数（size）超过临界值（threshold），就会进行自动扩容（resize），并且在扩容之后，还需要对HashMap中现有元素进行 “rehash” ，即将原来桶中的元素重新分配到新的桶中。

为什么是0.75呢？

在JDK的官方文档中有这样一段描述：

“As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put).”

大概意思是：一般来说，默认的负载因子（0.75）在时间和空间成本之间提供了很好的权衡。更高的值减少了空间开销，但增加了查找成本（反映在HashMap类的大多数操作中，包括get和put）。

试想一下，如果我们把负载因子设置成1，容量使用默认的初始值16，那么表示一个HashMap需要在 “满了” 之后才会进行扩容。

在HashMap中，最好的情况是这16个元素通过Hash计算后分别落到了16个不同的桶中，否则必然发生Hash碰撞。而且随着元素越多，Hash碰撞的概率越大，查找速度也会越低。

#### 1. 0.75的数学依据
我们可以通过一种数学方法来计算这个值是多少合适。

假设一个bucket为空和非空的概率为0.5，我们用s表示容量、n表示已添加元素的个数。根据二项式定理，桶为空的概率为：
```
P(0) = C(n, 0) * (1/s)^0 * (1 - 1/s)^(n - 0)
```
因此，如果桶中元素的个数小于以下数值，则桶可能是空的：
```
log(2)/log(s/(s - 1))
```
当s趋于无穷大时，如果增加的键的数量是P(0)=0.5，那么n/s很快趋近于log(2)：
```
log(2) ~ 0.693...
```
所以，合理值大概在0.7左右。
当然，这个数学方法并不是在Java的官方文档中体现的，我们也无从考证。这个推测来源于Stack Overflor（https://stackoverflow.com/questions/10901752/what-is-the-significance-of-load-factor-in-hashmap）。
#### 2. 0.75的必然因素
理
论上我们认为负载因子不能太大，不然会导致大量的Hash碰撞，也不能太小，那样会浪费空间。

通过上述的数学推理，测算出这个数值在0.7左右是比较合理的。

为什么最终选定了0.75呢？

根据HashMap的扩容机制，capacity的值永远都是2的幂。

为了保证负载因子（loadFactor）×容量（capacity）的结果是一个整数，这个值是0.75（3/4）比较合理，因为这个数和任何2的幂的乘积结果都是整数。

### 小结

负载因子表示一个Map可以达到的满的程度。这个值不宜太大，也不宜太小。

loadFactory太大，比如等于1，就会有很高的Hash冲突的概率，会大大降低查询速度。

loadFactory太小，比如等于0.5，那么频繁扩容会大大浪费空间。

所以，这个值需要介于0.5和1之间。根据数学公式推算，这个值为log2时比较合理。

另外，为了提升扩容效率，HashMap的容量（capacity）有一个固定的要求，那就是一定是2的幂。

所以，如果loadFactor是3/4，那么和capacity的乘积结果就可以是一个整数。

在一般情况下，我们不建议修改loadFactory的值。

比如明确地知道Map只存储5个键值对，并且永远不会改变，则可以考虑指定loadFactory的值。

其实我们完全可以通过指定capacity达到这样的目的。
### 10.19 HashMap的线程安全问题
前面介绍了HashMap，同时介绍了Hashtable和ConcurrentHashMap等，我们多次提到， 


HashMap是非线程安全的，是不可以用在并发场景中的。
为什么HashMap不能用在并发场景中呢？用了又会出现什么问题呢？
### 1. 扩容原理
10.12节简单介绍了HashMap的扩容机制，即当达到扩容条件时HashMap会进行扩容。前面还介绍了resize()方法中关于容量变化部分的代码，其中有一个重要的步骤没有介绍，那就是如何把原来Map中的元素移动到新的Map中。
下面是JDK 1.7中resize()方法的实现代码：
```java
void resize(int newCapacity) {
    Entry[] oldTable = table;
    int oldCapacity = oldTable.length;
    if (oldCapacity == MAXIMUM_CAPACITY) {
        threshold = Integer.MAX_VALUE;
        return;
    }
    Entry[] newTable = new Entry[newCapacity];
    boolean oldAltHashing = useAltHashing;
    useAltHashing |= sun.misc.VM.isBooted() &&
            (newCapacity >= Holder.ALTERNATIVE_HASHING_THRESHOLD);
    boolean rehash = oldAltHashing ^ useAltHashing;
    transfer(newTable, rehash);
    table = newTable;
    threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);
}
```
在上面的resize()方法中，调用了transfer()方法，这个方法实现的功能就是把原来的Map中元素移动到新的Map中，实现方式如下：
```java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```

首先解释这个方法做了哪些事情。

我们通过以下方式定义一个HashMap，设置其初始容量为4：
```java
Map<String,String> map = new HashMap<String,String>(3);
```

当我们使用3作为初始容量创建HashMap时，HashMap会采用第一个大于3的2的幂作为这个Map的初始容量，也就是4，而这个Map默认的负载因子是0.75，所以当元素个数超过3个（4×0.75）时，就会触发扩容机制。

我们依次向这个Map中添加3个元素：
```java
map.put("A","A");
map.put("B","B");
map.put("C","C");
```
如果这三个元素的Hash值刚好一样，那么它们的存储结构如图10 - 17所示。

![image](https://github.com/user-attachments/assets/16a4ff5a-cf34-414c-84d7-cc90ee54f966)


当我们向其中添加第四个元素D时，就会触发扩容机制。扩容过程就是先把容量变成原来的一倍，然后从原来的HashMap中依次取出元素再添加到扩容后的HashMap中。

transfer的元素移动的主要代码就是while循环中的这几句，为了便于读者理解，以下代码增加了一些注释：
```java
// 先保存下一个节点
Entry<K,V> next = e.next;
// 计算当前元素的Hash值
if (rehash) {
    e.hash = null == e.key? 0 : hash(e.key);
}
// 在新的Map中找到应该入的桶的下标
int i = indexFor(e.hash, newCapacity);
// 先用e.next指向新的Hash表的第一个元素，将当前元素插入链表的头部
e.next = newTable[i];
// 将新Hash表的头指针指向当前元素
newTable[i] = e;
// 转移到下一个节点
e = next;
```
如果A、B、C三个元素在HashMap扩容后还是一样的Hash值，那么它们会被分到同一个桶中。扩容后它们的存储结构如图10 - 18所示。

![image](https://github.com/user-attachments/assets/40b59252-3114-49e2-808c-04c333c50db2)



可以看到，它们之间的顺序从A→B→C变成了C→B→A。这就是所谓的头插法，即把元素插入链表头部。

之所以选择使用头插法，是因为JDK的开发者认为，后插入的数据被使用到的概率更高，更容易成为热点数据，而通过头插法把它们放在队列头部，就可以使查询效率更高。

介绍了HashMap中采用头插法进行扩容的机制之后，我们就可以分析HashMap在并发场景中存在的问题了。

其实也正是这个头插的过程，一旦出现高并发场景，就会出现死循环的问题。

接下来，我们就举一个实际的例子，重现一下上述情景。

### 2. 场景重现
同样还是Map中有A、B、C三个元素的情况，如图10 - 19所示。

![image](https://github.com/user-attachments/assets/5101c0a4-d33f-4693-97c5-ddbb43a3c186)



如果有多线程同时操作插入新的元素，就会同时触发resize方法，进而可能同时触发transfer方法。

在并发场景中，会发生怎样的情况呢？

假设线程1在扩容过程中，在执行Entry<K,V> next = e.next后，失去了CPU时间片。

为了看起来像是 “同时做许多事” ，现代分时操作系统把CPU的时间划分为长短基本相同的时间区间，即 “时间片” ，通过操作系统的管理，把这些时间片依次轮流地分配给各个 “用户” 使用。如果某个 “用户” 在时间片结束之前，整个任务还没有完成，那么 “用户” 就必须进入就绪状态，放弃使用CPU，等待下一轮循环。此时CPU又被分配给另一个 “用户” 使用。

这时线程A中的数据结构如图10 - 20所示。

![image](https://github.com/user-attachments/assets/4873c2e5-d020-4064-8c46-63dade0a1b26)


在线程1失去时间片之后，线程2获取了CPU时间片，并把扩容操作执行完，这时的情况如图10 - 21所示。

![image](https://github.com/user-attachments/assets/ede0da00-2e07-40f0-bae1-a2a3ea72a856)


对于线程1来说，它看到的情况如图10 - 22所示。


![image](https://github.com/user-attachments/assets/986ee5ba-2786-49f2-a3d3-ce9c6450e5f9)



这时线程1重新获得CPU时间片，并接着执行代码。

当执行到代码newTable[i] = e时的情况如图10 - 23所示。

![image](https://github.com/user-attachments/assets/9e7aa4e9-a4ae-4d5a-b9a6-8ea2622fb663)


接下来执行e = next，这时的情况如图10 - 24所示。


![image](https://github.com/user-attachments/assets/67e45ba2-e155-44a2-b68f-ab80c9fa70bc)


接下来，开始执行下一轮的while循环，执行Entry next = e.next，这时的情况如图10 - 25所示。

![image](https://github.com/user-attachments/assets/3aa6f675-b3e9-4c46-ba52-eac6b0de6db5)


接着执行e.next = newTable[i]; newTable[i] = e，这时的情况如图10 - 26所示。

![image](https://github.com/user-attachments/assets/e35d4ca1-446f-4d9d-911f-5760586fa44d)


继续执行e=next，这时的情况如图10 - 27所示。

![image](https://github.com/user-attachments/assets/224bb459-5129-467d-9b39-719b788049c7)


接着开始执行新一轮的while循环，当执行到Entry<K,V> next = e.next时的情况如图10 - 28所示。

![image](https://github.com/user-attachments/assets/47c60525-2595-4923-9c42-2720d888ece0)


继续执行e.next = newTable[i]，这时的情况如图10 - 29所示。

至此，就产生了一个循环链表，这时如果在线程1中执行查询操作，就会陷入死循环，直到CPU资源被耗尽。 

![image](https://github.com/user-attachments/assets/21ba6f7e-d5c6-4316-aa01-5d1c3694fa4b)


### 小结

本节介绍了HashMap在并发情况下，因为扩容而导致的死循环现象。所以，在日常开发中，一定要尽量避免在并发环境中使用HashMap，而是使用Hashtable、ConcurrentHashMap等代替HashMap。

前面提到，之所以会发生这个死循环问题，是因为在JDK 1.8之前的版本中，HashMap是采用头插法进行扩容的，这个问题其实在JDK 1.8中已经被修复了。JDK 1.8中的resize代码如下：
```java
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null)? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    if (oldCap > 0) {
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    else {               // zero initial threshold signifies using defaults
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY?
                  (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;
    @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    if (oldTab != null) {
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else { // preserve order
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```
需要强调的是，虽然这个死循环的问题被修复了，但HashMap还是不适用于并发场景，在并发情况下可能出现丢失数据等情况。 
